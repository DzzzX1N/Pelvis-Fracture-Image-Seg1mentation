{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X-Ray Segmentation using U-Net\n",
    "\n",
    "This notebook implements pelvis X-ray segmentation using a U-Net model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MarkXu\\anaconda3\\envs\\pelvis_seg\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4070\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from src.utils.pengwin_utils import (\n",
    "    load_image, \n",
    "    load_masks, \n",
    "    build_augmentation,\n",
    "    visualize_sample,\n",
    "    CATEGORIES\n",
    ")\n",
    "\n",
    "# Print versions for reproducibility\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Implementation\n",
    "First, let's create our custom dataset class for handling X-ray images and their masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XrayDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', img_size=448):\n",
    "        self.root = Path(root_dir)\n",
    "        self.split = split\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # Setup directories\n",
    "        self.input_dir = self.root / split / \"input\" / \"images\" / \"x-ray\"\n",
    "        self.output_dir = self.root / split / \"output\" / \"images\" / \"x-ray\"\n",
    "        \n",
    "        # Get file paths\n",
    "        self.image_paths = sorted(self.input_dir.glob(\"*.tif\"))\n",
    "        self.mask_paths = sorted(self.output_dir.glob(\"*.tif\"))\n",
    "        \n",
    "        assert len(self.image_paths) == len(self.mask_paths)\n",
    "        \n",
    "        # Setup augmentation\n",
    "        self.aug = build_augmentation(train=(split=='train'), img_size=img_size)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and mask\n",
    "        image = load_image(self.image_paths[idx])\n",
    "        masks, category_ids, _ = load_masks(self.mask_paths[idx])\n",
    "        \n",
    "        # Apply augmentation\n",
    "        augmented = self.aug(image=image, masks=masks)\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        image = torch.from_numpy(augmented['image']).float()\n",
    "        masks = torch.from_numpy(np.array(augmented['masks'])).float()\n",
    "        \n",
    "        return image, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Lambda.__init__() got an unexpected keyword argument 'keypoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mXrayDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxray_seg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Test loading one sample\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m, in \u001b[0;36mXrayDataset.__init__\u001b[1;34m(self, root_dir, split, img_size)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_paths)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Setup augmentation\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maug \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_augmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\pelvis\\xray_seg\\src\\utils\\pengwin_utils.py:255\u001b[0m, in \u001b[0;36mbuild_augmentation\u001b[1;34m(train, img_size)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m train:\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m A\u001b[38;5;241m.\u001b[39mCompose(\n\u001b[0;32m    250\u001b[0m         [neglog_aug(), window(\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.95\u001b[39m, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), A\u001b[38;5;241m.\u001b[39mResize(img_size, img_size)]\n\u001b[0;32m    251\u001b[0m     )\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m A\u001b[38;5;241m.\u001b[39mCompose(\n\u001b[0;32m    254\u001b[0m     [\n\u001b[1;32m--> 255\u001b[0m         \u001b[43mneglog_aug\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    256\u001b[0m         window((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.05\u001b[39m), (\u001b[38;5;241m0.95\u001b[39m, \u001b[38;5;241m1.0\u001b[39m), convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    257\u001b[0m         A\u001b[38;5;241m.\u001b[39mResize(img_size, img_size),\n\u001b[0;32m    258\u001b[0m         A\u001b[38;5;241m.\u001b[39mCLAHE(clip_limit\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m), p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[0;32m    259\u001b[0m         A\u001b[38;5;241m.\u001b[39mInvertImg(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[0;32m    260\u001b[0m         A\u001b[38;5;241m.\u001b[39mSomeOf(\n\u001b[0;32m    261\u001b[0m             [\n\u001b[0;32m    262\u001b[0m                 A\u001b[38;5;241m.\u001b[39mOneOf(\n\u001b[0;32m    263\u001b[0m                     [\n\u001b[0;32m    264\u001b[0m                         A\u001b[38;5;241m.\u001b[39mGaussianBlur((\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m)),\n\u001b[0;32m    265\u001b[0m                         A\u001b[38;5;241m.\u001b[39mMotionBlur(blur_limit\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m)),\n\u001b[0;32m    266\u001b[0m                         A\u001b[38;5;241m.\u001b[39mMedianBlur(blur_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m),\n\u001b[0;32m    267\u001b[0m                     ],\n\u001b[0;32m    268\u001b[0m                 ),\n\u001b[0;32m    269\u001b[0m                 A\u001b[38;5;241m.\u001b[39mOneOf(\n\u001b[0;32m    270\u001b[0m                     [\n\u001b[0;32m    271\u001b[0m                         A\u001b[38;5;241m.\u001b[39mSharpen(alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.5\u001b[39m)),\n\u001b[0;32m    272\u001b[0m                         A\u001b[38;5;241m.\u001b[39mEmboss(alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.5\u001b[39m)),\n\u001b[0;32m    273\u001b[0m                     ],\n\u001b[0;32m    274\u001b[0m                 ),\n\u001b[0;32m    275\u001b[0m                 A\u001b[38;5;241m.\u001b[39mOneOf(\n\u001b[0;32m    276\u001b[0m                     [\n\u001b[0;32m    277\u001b[0m                         A\u001b[38;5;241m.\u001b[39mMultiplicativeNoise(multiplier\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m1.1\u001b[39m)),\n\u001b[0;32m    278\u001b[0m                         A\u001b[38;5;241m.\u001b[39mHueSaturationValue(\n\u001b[0;32m    279\u001b[0m                             hue_shift_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m    280\u001b[0m                             sat_shift_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m    281\u001b[0m                             val_shift_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m    282\u001b[0m                         ),\n\u001b[0;32m    283\u001b[0m                         A\u001b[38;5;241m.\u001b[39mRandomBrightnessContrast(\n\u001b[0;32m    284\u001b[0m                             brightness_limit\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;241m0.2\u001b[39m), contrast_limit\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m    285\u001b[0m                         ),\n\u001b[0;32m    286\u001b[0m                         gaussian_contrast_aug(\n\u001b[0;32m    287\u001b[0m                             alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m1.4\u001b[39m), sigma\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.5\u001b[39m), max_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m\n\u001b[0;32m    288\u001b[0m                         ),\n\u001b[0;32m    289\u001b[0m                     ],\n\u001b[0;32m    290\u001b[0m                 ),\n\u001b[0;32m    291\u001b[0m                 A\u001b[38;5;241m.\u001b[39mRandomToneCurve(scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m),\n\u001b[0;32m    292\u001b[0m                 A\u001b[38;5;241m.\u001b[39mOneOf(\n\u001b[0;32m    293\u001b[0m                     [\n\u001b[0;32m    294\u001b[0m                         A\u001b[38;5;241m.\u001b[39mRandomShadow(),\n\u001b[0;32m    295\u001b[0m                         A\u001b[38;5;241m.\u001b[39mRandomFog(fog_coef_lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, fog_coef_upper\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, alpha_coef\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.08\u001b[39m),\n\u001b[0;32m    296\u001b[0m                     ],\n\u001b[0;32m    297\u001b[0m                 ),\n\u001b[0;32m    298\u001b[0m                 A\u001b[38;5;241m.\u001b[39mOneOf(\n\u001b[0;32m    299\u001b[0m                     [\n\u001b[0;32m    300\u001b[0m                         Dropout(dropout_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m),\n\u001b[0;32m    301\u001b[0m                         CoarseDropout(\n\u001b[0;32m    302\u001b[0m                             max_holes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m,\n\u001b[0;32m    303\u001b[0m                             max_height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m,\n\u001b[0;32m    304\u001b[0m                             max_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m,\n\u001b[0;32m    305\u001b[0m                             min_holes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m    306\u001b[0m                             min_height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m    307\u001b[0m                             min_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m    308\u001b[0m                         ),\n\u001b[0;32m    309\u001b[0m                     ],\n\u001b[0;32m    310\u001b[0m                     p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m    311\u001b[0m                 ),\n\u001b[0;32m    312\u001b[0m             ],\n\u001b[0;32m    313\u001b[0m             n\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m),\n\u001b[0;32m    314\u001b[0m             replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    315\u001b[0m         ),\n\u001b[0;32m    316\u001b[0m         A\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], max_pixel_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m),  \u001b[38;5;66;03m# Normalize to [0, 1]\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     ],\n\u001b[0;32m    318\u001b[0m )\n",
      "File \u001b[1;32md:\\pelvis\\xray_seg\\src\\utils\\pengwin_utils.py:161\u001b[0m, in \u001b[0;36mneglog_aug\u001b[1;34m(epsilon)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf_id\u001b[39m(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLambda\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeypoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mneglog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Lambda.__init__() got an unexpected keyword argument 'keypoint'"
     ]
    }
   ],
   "source": [
    "# Initialize dataset\n",
    "dataset = XrayDataset('xray_seg', 'train')\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Test loading one sample\n",
    "image, masks = dataset[0]\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "print(f\"Masks shape: {masks.shape}\")\n",
    "\n",
    "# Visualize sample\n",
    "vis_img = visualize_sample(\n",
    "    image.numpy(), \n",
    "    masks.numpy(), \n",
    "    category_ids=list(CATEGORIES.values()),\n",
    "    fragment_ids=[1]*len(CATEGORIES)\n",
    ")\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(vis_img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. U-Net Model Implementation\n",
    "Now let's implement our U-Net architecture for segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder (Contracting Path)\n",
    "        self.enc1 = DoubleConv(in_channels, 64)\n",
    "        self.enc2 = DoubleConv(64, 128)\n",
    "        self.enc3 = DoubleConv(128, 256)\n",
    "        self.enc4 = DoubleConv(256, 512)\n",
    "        self.enc5 = DoubleConv(512, 1024)\n",
    "        \n",
    "        # Decoder (Expansive Path)\n",
    "        self.dec4 = DoubleConv(1024 + 512, 512)\n",
    "        self.dec3 = DoubleConv(512 + 256, 256)\n",
    "        self.dec2 = DoubleConv(256 + 128, 128)\n",
    "        self.dec1 = DoubleConv(128 + 64, 64)\n",
    "        \n",
    "        # Final convolution\n",
    "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "        \n",
    "        # Pooling and Upsampling\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(self.pool(enc1))\n",
    "        enc3 = self.enc3(self.pool(enc2))\n",
    "        enc4 = self.enc4(self.pool(enc3))\n",
    "        enc5 = self.enc5(self.pool(enc4))\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        dec4 = self.dec4(torch.cat([self.up(enc5), enc4], dim=1))\n",
    "        dec3 = self.dec3(torch.cat([self.up(dec4), enc3], dim=1))\n",
    "        dec2 = self.dec2(torch.cat([self.up(dec3), enc2], dim=1))\n",
    "        dec1 = self.dec1(torch.cat([self.up(dec2), enc1], dim=1))\n",
    "        \n",
    "        return self.final_conv(dec1)\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNet(in_channels=1, out_channels=len(CATEGORIES)).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "print(f\"Model initialized on: {device}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with tqdm(dataloader, desc='Training') as pbar:\n",
    "        for images, masks in pbar:\n",
    "            # Move to device\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update progress bar\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(dataloader, desc='Validation') as pbar:\n",
    "            for images, masks in pbar:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return val_loss / len(dataloader)\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = 100\n",
    "batch_size = 8\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataset = XrayDataset('path/to/your/data', split='train')\n",
    "val_dataset = XrayDataset('path/to/your/data', split='val')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f'Training Loss: {train_loss:.4f}')\n",
    "    print(f'Validation Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "        }, 'best_model.pth')\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pelvis_seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
