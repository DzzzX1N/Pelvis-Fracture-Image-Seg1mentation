{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net 3D CT Segmentation\n",
    "\n",
    "This notebook implements a 3D U-Net model for segmenting pelvis from CT scans.\n",
    "\n",
    "## Setup and Imports\n",
    "\n",
    "First, let's check our environment and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1\n",
      "CUDA available: False\n",
      "SimpleITK version: 2.4.0\n",
      "NumPy version: 1.24.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Print versions for reproducibility\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"SimpleITK version: {sitk.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration\n",
    "Let's examine our dataset structure and visualize some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 image files in ../data\\PENGWIN_CT_train_images\n",
      "Found 100 label files in ../data\\PENGWIN_CT_train_labels\n",
      "\n",
      "First 5 image files:\n",
      "- 001.mha\n",
      "- 002.mha\n",
      "- 003.mha\n",
      "- 004.mha\n",
      "- 005.mha\n",
      "\n",
      "Original image dimensions: (401, 512, 512)\n",
      "Image spacing: (0.78125, 0.78125, 0.800000011920929)\n",
      "Value range: [-1023.00, 2775.00] HU\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "data_dir = '../data'\n",
    "images_path = os.path.join(data_dir, 'PENGWIN_CT_train_images')\n",
    "labels_path = os.path.join(data_dir, 'PENGWIN_CT_train_labels')\n",
    "\n",
    "# List and count files\n",
    "image_files = sorted([f for f in os.listdir(images_path) if f.endswith('.mha')])\n",
    "label_files = sorted([f for f in os.listdir(labels_path) if f.endswith('.mha')])\n",
    "\n",
    "print(f\"Found {len(image_files)} image files in {images_path}\")\n",
    "print(f\"Found {len(label_files)} label files in {labels_path}\")\n",
    "print(\"\\nFirst 5 image files:\")\n",
    "for f in image_files[:5]:\n",
    "    print(f\"- {f}\")\n",
    "\n",
    "# Load one sample to check dimensions\n",
    "sample_image = sitk.ReadImage(os.path.join(images_path, image_files[0]))\n",
    "sample_array = sitk.GetArrayFromImage(sample_image)\n",
    "print(f\"\\nOriginal image dimensions: {sample_array.shape}\")\n",
    "print(f\"Image spacing: {sample_image.GetSpacing()}\")\n",
    "print(f\"Value range: [{sample_array.min():.2f}, {sample_array.max():.2f}] HU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTScanDataset(Dataset):\n",
    "    def __init__(self, images_path, labels_path, patch_size=(128, 128, 128), stride=(64, 64, 64)):\n",
    "        self.image_paths = sorted([os.path.join(images_path, fname) \n",
    "                                for fname in os.listdir(images_path) \n",
    "                                if fname.endswith('.mha')])\n",
    "        self.label_paths = sorted([os.path.join(labels_path, fname) \n",
    "                                for fname in os.listdir(labels_path) \n",
    "                                if fname.endswith('.mha')])\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        \n",
    "        assert len(self.image_paths) == len(self.label_paths)\n",
    "\n",
    "    def extract_patches(self, image, label):\n",
    "        \"\"\"Extract patches from image and label\"\"\"\n",
    "        patches_img = []\n",
    "        patches_label = []\n",
    "        \n",
    "        D, H, W = image.shape\n",
    "        \n",
    "        # Calculate steps for each dimension\n",
    "        d_steps = range(0, D - self.patch_size[0] + 1, self.stride[0])\n",
    "        h_steps = range(0, H - self.patch_size[1] + 1, self.stride[1])\n",
    "        w_steps = range(0, W - self.patch_size[2] + 1, self.stride[2])\n",
    "        \n",
    "        # If image is smaller than patch size, pad it\n",
    "        if D < self.patch_size[0]:\n",
    "            d_steps = [0]\n",
    "        if H < self.patch_size[1]:\n",
    "            h_steps = [0]\n",
    "        if W < self.patch_size[2]:\n",
    "            w_steps = [0]\n",
    "            \n",
    "        for d in d_steps:\n",
    "            for h in h_steps:\n",
    "                for w in w_steps:\n",
    "                    # Extract patches\n",
    "                    d_end = min(d + self.patch_size[0], D)\n",
    "                    h_end = min(h + self.patch_size[1], H)\n",
    "                    w_end = min(w + self.patch_size[2], W)\n",
    "                    \n",
    "                    patch_img = image[d:d_end, h:h_end, w:w_end]\n",
    "                    patch_label = label[d:d_end, h:h_end, w:w_end]\n",
    "                    \n",
    "                    # Pad if necessary\n",
    "                    if patch_img.shape != self.patch_size:\n",
    "                        pad_d = self.patch_size[0] - patch_img.shape[0]\n",
    "                        pad_h = self.patch_size[1] - patch_img.shape[1]\n",
    "                        pad_w = self.patch_size[2] - patch_img.shape[2]\n",
    "                        \n",
    "                        patch_img = np.pad(patch_img, \n",
    "                                         ((0, pad_d), (0, pad_h), (0, pad_w)), \n",
    "                                         mode='constant')\n",
    "                        patch_label = np.pad(patch_label, \n",
    "                                           ((0, pad_d), (0, pad_h), (0, pad_w)), \n",
    "                                           mode='constant')\n",
    "                    \n",
    "                    patches_img.append(patch_img)\n",
    "                    patches_label.append(patch_label)\n",
    "        \n",
    "        return np.array(patches_img), np.array(patches_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and label\n",
    "        image = sitk.GetArrayFromImage(sitk.ReadImage(self.image_paths[idx])).astype(np.float32)\n",
    "        label = sitk.GetArrayFromImage(sitk.ReadImage(self.label_paths[idx])).astype(np.float32)\n",
    "        \n",
    "        # Preprocessing\n",
    "        image = np.clip(image, -1000, 1000)\n",
    "        image = (image + 1000) / 2000\n",
    "        label = (label > 0).astype(np.float32)\n",
    "        \n",
    "        # Extract patches\n",
    "        patches_img, patches_label = self.extract_patches(image, label)\n",
    "        \n",
    "        # Randomly select one patch during training\n",
    "        patch_idx = np.random.randint(len(patches_img))\n",
    "        \n",
    "        # Add channel dimension\n",
    "        image_patch = np.expand_dims(patches_img[patch_idx], axis=0)\n",
    "        label_patch = np.expand_dims(patches_label[patch_idx], axis=0)\n",
    "        \n",
    "        return torch.tensor(image_patch), torch.tensor(label_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_volume(model, image, patch_size=(128, 128, 128), stride=(64, 64, 64)):\n",
    "    \"\"\"Predict segmentation using sliding window approach\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    D, H, W = image.shape\n",
    "    output = np.zeros_like(image, dtype=np.float32)\n",
    "    weight = np.zeros_like(image, dtype=np.float32)\n",
    "    \n",
    "    # Calculate steps\n",
    "    d_steps = range(0, D - patch_size[0] + 1, stride[0])\n",
    "    h_steps = range(0, H - patch_size[1] + 1, stride[1])\n",
    "    w_steps = range(0, W - patch_size[2] + 1, stride[2])\n",
    "    \n",
    "    # Handle edge cases\n",
    "    if D < patch_size[0]:\n",
    "        d_steps = [0]\n",
    "    if H < patch_size[1]:\n",
    "        h_steps = [0]\n",
    "    if W < patch_size[2]:\n",
    "        w_steps = [0]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d in d_steps:\n",
    "            for h in h_steps:\n",
    "                for w in w_steps:\n",
    "                    # Extract patch\n",
    "                    d_end = min(d + patch_size[0], D)\n",
    "                    h_end = min(h + patch_size[1], H)\n",
    "                    w_end = min(w + patch_size[2], W)\n",
    "                    \n",
    "                    patch = image[d:d_end, h:h_end, w:w_end]\n",
    "                    \n",
    "                    # Pad if necessary\n",
    "                    if patch.shape != patch_size:\n",
    "                        pad_d = patch_size[0] - patch.shape[0]\n",
    "                        pad_h = patch_size[1] - patch.shape[1]\n",
    "                        pad_w = patch_size[2] - patch.shape[2]\n",
    "                        \n",
    "                        patch = np.pad(patch, \n",
    "                                     ((0, pad_d), (0, pad_h), (0, pad_w)), \n",
    "                                     mode='constant')\n",
    "                    \n",
    "                    # Predict\n",
    "                    patch = torch.tensor(patch).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "                    pred = model(patch).cpu().numpy()[0, 0]\n",
    "                    \n",
    "                    # Unpad if necessary\n",
    "                    if pad_d > 0 or pad_h > 0 or pad_w > 0:\n",
    "                        pred = pred[:d_end-d, :h_end-h, :w_end-w]\n",
    "                    \n",
    "                    # Add to output with weight\n",
    "                    output[d:d_end, h:h_end, w:w_end] += pred\n",
    "                    weight[d:d_end, h:h_end, w:w_end] += 1\n",
    "    \n",
    "    # Average overlapping predictions\n",
    "    output = np.divide(output, weight, where=weight!=0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet3D, self).__init__()\n",
    "        \n",
    "        def conv_block(in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv3d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv3d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        \n",
    "        def up_conv(in_ch, out_ch):\n",
    "            return nn.ConvTranspose3d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder1 = conv_block(in_channels, 64)\n",
    "        self.encoder2 = conv_block(64, 128)\n",
    "        self.encoder3 = conv_block(128, 256)\n",
    "        self.encoder4 = conv_block(256, 512)\n",
    "        \n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = conv_block(512, 1024)\n",
    "        \n",
    "        # Decoder\n",
    "        self.upconv4 = up_conv(1024, 512)\n",
    "        self.decoder4 = conv_block(1024, 512)\n",
    "        self.upconv3 = up_conv(512, 256)\n",
    "        self.decoder3 = conv_block(512, 256)\n",
    "        self.upconv2 = up_conv(256, 128)\n",
    "        self.decoder2 = conv_block(256, 128)\n",
    "        self.upconv1 = up_conv(128, 64)\n",
    "        self.decoder1 = conv_block(128, 64)\n",
    "        \n",
    "        self.output = nn.Conv3d(64, out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Implementation remains the same as in your original code\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(self.pool(e1))\n",
    "        e3 = self.encoder3(self.pool(e2))\n",
    "        e4 = self.encoder4(self.pool(e3))\n",
    "        \n",
    "        b = self.bottleneck(self.pool(e4))\n",
    "        \n",
    "        d4 = self.upconv4(b)\n",
    "        d4 = self.decoder4(torch.cat((e4, d4), dim=1))\n",
    "        d3 = self.upconv3(d4)\n",
    "        d3 = self.decoder3(torch.cat((e3, d3), dim=1))\n",
    "        d2 = self.upconv2(d3)\n",
    "        d2 = self.decoder2(torch.cat((e2, d2), dim=1))\n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = self.decoder1(torch.cat((e1, d1), dim=1))\n",
    "        \n",
    "        return self.output(d1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show the parameter of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture:\n",
      "------------------\n",
      "encoder1.0.weight: [64, 1, 3, 3, 3] (1,728 parameters)\n",
      "encoder1.0.bias: [64] (64 parameters)\n",
      "encoder1.2.weight: [64, 64, 3, 3, 3] (110,592 parameters)\n",
      "encoder1.2.bias: [64] (64 parameters)\n",
      "encoder2.0.weight: [128, 64, 3, 3, 3] (221,184 parameters)\n",
      "encoder2.0.bias: [128] (128 parameters)\n",
      "encoder2.2.weight: [128, 128, 3, 3, 3] (442,368 parameters)\n",
      "encoder2.2.bias: [128] (128 parameters)\n",
      "encoder3.0.weight: [256, 128, 3, 3, 3] (884,736 parameters)\n",
      "encoder3.0.bias: [256] (256 parameters)\n",
      "encoder3.2.weight: [256, 256, 3, 3, 3] (1,769,472 parameters)\n",
      "encoder3.2.bias: [256] (256 parameters)\n",
      "encoder4.0.weight: [512, 256, 3, 3, 3] (3,538,944 parameters)\n",
      "encoder4.0.bias: [512] (512 parameters)\n",
      "encoder4.2.weight: [512, 512, 3, 3, 3] (7,077,888 parameters)\n",
      "encoder4.2.bias: [512] (512 parameters)\n",
      "bottleneck.0.weight: [1024, 512, 3, 3, 3] (14,155,776 parameters)\n",
      "bottleneck.0.bias: [1024] (1,024 parameters)\n",
      "bottleneck.2.weight: [1024, 1024, 3, 3, 3] (28,311,552 parameters)\n",
      "bottleneck.2.bias: [1024] (1,024 parameters)\n",
      "upconv4.weight: [1024, 512, 2, 2, 2] (4,194,304 parameters)\n",
      "upconv4.bias: [512] (512 parameters)\n",
      "decoder4.0.weight: [512, 1024, 3, 3, 3] (14,155,776 parameters)\n",
      "decoder4.0.bias: [512] (512 parameters)\n",
      "decoder4.2.weight: [512, 512, 3, 3, 3] (7,077,888 parameters)\n",
      "decoder4.2.bias: [512] (512 parameters)\n",
      "upconv3.weight: [512, 256, 2, 2, 2] (1,048,576 parameters)\n",
      "upconv3.bias: [256] (256 parameters)\n",
      "decoder3.0.weight: [256, 512, 3, 3, 3] (3,538,944 parameters)\n",
      "decoder3.0.bias: [256] (256 parameters)\n",
      "decoder3.2.weight: [256, 256, 3, 3, 3] (1,769,472 parameters)\n",
      "decoder3.2.bias: [256] (256 parameters)\n",
      "upconv2.weight: [256, 128, 2, 2, 2] (262,144 parameters)\n",
      "upconv2.bias: [128] (128 parameters)\n",
      "decoder2.0.weight: [128, 256, 3, 3, 3] (884,736 parameters)\n",
      "decoder2.0.bias: [128] (128 parameters)\n",
      "decoder2.2.weight: [128, 128, 3, 3, 3] (442,368 parameters)\n",
      "decoder2.2.bias: [128] (128 parameters)\n",
      "upconv1.weight: [128, 64, 2, 2, 2] (65,536 parameters)\n",
      "upconv1.bias: [64] (64 parameters)\n",
      "decoder1.0.weight: [64, 128, 3, 3, 3] (221,184 parameters)\n",
      "decoder1.0.bias: [64] (64 parameters)\n",
      "decoder1.2.weight: [64, 64, 3, 3, 3] (110,592 parameters)\n",
      "decoder1.2.bias: [64] (64 parameters)\n",
      "output.weight: [1, 64, 1, 1, 1] (64 parameters)\n",
      "output.bias: [1] (1 parameters)\n",
      "\n",
      "Total parameters: 90,292,673\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    def print_model_summary(model):\n",
    "        \"\"\"Print model architecture summary\"\"\"\n",
    "        print(\"Model Architecture:\")\n",
    "        print(\"------------------\")\n",
    "        total_params = 0\n",
    "        for name, param in model.named_parameters():\n",
    "            param_count = param.numel()\n",
    "            total_params += param_count\n",
    "            print(f\"{name}: {list(param.shape)} ({param_count:,} parameters)\")\n",
    "        print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "    # Initialize model and print summary\n",
    "    model = UNet3D(in_channels=1, out_channels=1)\n",
    "    print_model_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Configuration and Progress Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Paths\n",
    "data_dir = r'..\\data'\n",
    "images_path = os.path.join(data_dir, 'PENGWIN_CT_train_images')\n",
    "labels_path = os.path.join(data_dir, 'PENGWIN_CT_train_labels')\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# Dataset and DataLoader setup\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: torch.tensor(x, dtype=torch.float32)),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Initialize dataset with patch-based approach\n",
    "train_dataset = CTScanDataset(\n",
    "    images_path=images_path,\n",
    "    labels_path=labels_path,\n",
    "    patch_size=(128, 128, 128),\n",
    "    stride=(64, 64, 64)\n",
    ")\n",
    "\n",
    "# Rest of your training code remains the same\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Model initialization\n",
    "model = UNet3D(in_channels=1, out_channels=1).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "class TrainingMonitor:\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "    def update(self, epoch_loss):\n",
    "        self.train_losses.append(epoch_loss)\n",
    "        self.current_epoch += 1\n",
    "        \n",
    "    def plot_progress(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.train_losses, label='Training Loss')\n",
    "        plt.title('Training Progress')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def print_stats(self):\n",
    "        print(f\"Current epoch: {self.current_epoch}\")\n",
    "        print(f\"Best loss: {min(self.train_losses):.4f}\")\n",
    "        print(f\"Current loss: {self.train_losses[-1]:.4f}\")\n",
    "\n",
    "# Initialize training monitor\n",
    "monitor = TrainingMonitor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 50/50 [1:02:24<00:00, 74.89s/it, loss=0.158] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model with loss: 1.4290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 50/50 [46:33<00:00, 55.87s/it, loss=0.0919]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model with loss: 0.1748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 50/50 [41:34<00:00, 49.90s/it, loss=0.277]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new best model with loss: 0.1197\n"
     ]
    }
   ],
   "source": [
    "# 在训练循环之前，添加保存路径\n",
    "MODEL_SAVE_PATH = 'best_unet_model.pth'  # 保存最佳模型\n",
    "CHECKPOINT_DIR = 'checkpoints'  # 保存检查点的目录\n",
    "\n",
    "# 创建检查点目录\n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    os.makedirs(CHECKPOINT_DIR)\n",
    "\n",
    "def validate(model, val_image_path, val_label_path):\n",
    "    \"\"\"Validate model on a single volume\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Load validation image and label\n",
    "    image = sitk.GetArrayFromImage(sitk.ReadImage(val_image_path)).astype(np.float32)\n",
    "    label = sitk.GetArrayFromImage(sitk.ReadImage(val_label_path)).astype(np.float32)\n",
    "    \n",
    "    # Preprocess\n",
    "    image = np.clip(image, -1000, 1000)\n",
    "    image = (image + 1000) / 2000\n",
    "    label = (label > 0).astype(np.float32)\n",
    "    \n",
    "    # Predict using sliding window\n",
    "    pred = predict_volume(model, image)\n",
    "    \n",
    "    # Calculate Dice score\n",
    "    pred_binary = (pred > 0.5).astype(np.float32)\n",
    "    intersection = np.sum(pred_binary * label)\n",
    "    dice = (2. * intersection) / (np.sum(pred_binary) + np.sum(label) + 1e-8)\n",
    "    \n",
    "    return dice\n",
    "\n",
    "# Modify your training loop to include validation\n",
    "def train_model(model, train_loader, val_image_path, val_label_path, criterion, optimizer, num_epochs, device, monitor):\n",
    "    best_loss = float('inf')\n",
    "    best_dice = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Training loop\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for images, masks in pbar:\n",
    "            images = images.float().to(device)\n",
    "            masks = masks.float().to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        # Validation\n",
    "        if (epoch + 1) % 5 == 0:  # Validate every 5 epochs\n",
    "            dice_score = validate(model, val_image_path, val_label_path)\n",
    "            print(f\"Validation Dice score: {dice_score:.4f}\")\n",
    "            \n",
    "            # Save best model based on dice score\n",
    "            if dice_score > best_dice:\n",
    "                best_dice = dice_score\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'dice_score': dice_score,\n",
    "                }, 'best_model_dice.pth')\n",
    "                print(f\"Saved new best model with Dice score: {dice_score:.4f}\")\n",
    "                \n",
    "        # Calculate average loss\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        monitor.update(avg_loss)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_loss,\n",
    "            }, 'best_model_loss.pth')\n",
    "            print(f\"Saved new best model with loss: {best_loss:.4f}\")\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            monitor.plot_progress()\n",
    "            monitor.print_stats()\n",
    "\n",
    "# Split your data into training and validation sets\n",
    "all_image_files = sorted(os.listdir(images_path))\n",
    "all_label_files = sorted(os.listdir(labels_path))\n",
    "\n",
    "# Use last image for validation\n",
    "val_image_path = os.path.join(images_path, all_image_files[-1])\n",
    "val_label_path = os.path.join(labels_path, all_label_files[-1])\n",
    "\n",
    "\n",
    "# 如果要继续训练，添加这个函数\n",
    "def continue_training(model, optimizer, val_image_path, val_label_path, start_epoch, num_epochs):\n",
    "    checkpoint = torch.load('interrupted_model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    \n",
    "    print(f\"Continuing training from epoch {start_epoch+1}\")\n",
    "    train_model(model, train_loader, val_image_path, val_label_path,\n",
    "                criterion, optimizer, num_epochs-start_epoch, device, monitor)\n",
    "\n",
    "# 使用方法：\n",
    "# Train the model with validation\n",
    "train_model(model, train_loader, val_image_path, val_label_path, \n",
    "           criterion, optimizer, NUM_EPOCHS, device, monitor)\n",
    "\n",
    "# 2. 如果训练被中断，之后可以继续训练\n",
    "# continue_training(model, train_loader, val_image_path, val_label_path, criterion, optimizer, NUM_EPOCHS, device, monitor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMonitor:\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.val_dices = []\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "    def update(self, epoch_loss, val_dice=None):\n",
    "        self.train_losses.append(epoch_loss)\n",
    "        if val_dice is not None:\n",
    "            self.val_dices.append(val_dice)\n",
    "        self.current_epoch += 1\n",
    "        \n",
    "    def plot_progress(self):\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Plot training loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.train_losses, label='Training Loss')\n",
    "        plt.title('Training Loss Over Time')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot validation dice if available\n",
    "        if self.val_dices:\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(range(0, len(self.val_dices)*5, 5), self.val_dices, label='Validation Dice')\n",
    "            plt.title('Validation Dice Over Time')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Dice Score')\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pelvis_seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
