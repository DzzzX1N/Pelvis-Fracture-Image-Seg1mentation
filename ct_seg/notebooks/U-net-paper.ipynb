{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net 3D Paper CT Segmentation\n",
    "\n",
    "This notebook implements a 3D U-Net model for segmenting pelvis from CT scans.\n",
    "\n",
    "## Setup and Imports\n",
    "\n",
    "First, let's check our environment and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SimpleITK in c:\\users\\markxu\\anaconda3\\envs\\pelvis_seg\\lib\\site-packages (2.4.0)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Print versions for reproducibility\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "!pip install SimpleITK\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from google.colab import drive\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "# Print versions for reproducibility\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"SimpleITK version: {sitk.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration\n",
    "Let's examine our dataset structure and visualize some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 image files in ../data\\PENGWIN_CT_train_images\n",
      "Found 100 label files in ../data\\PENGWIN_CT_train_labels\n",
      "\n",
      "First 5 image files:\n",
      "- 001.mha\n",
      "- 002.mha\n",
      "- 003.mha\n",
      "- 004.mha\n",
      "- 005.mha\n",
      "\n",
      "Original image dimensions: (401, 512, 512)\n",
      "Image spacing: (0.78125, 0.78125, 0.800000011920929)\n",
      "Value range: [-1023.00, 2775.00] HU\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "drive.mount('/content/drive')\n",
    "data_dir = '/content/drive/MyDrive/data'\n",
    "BASE_DIR = '/content/drive/MyDrive/ct_segmentation'  # Create a dedicated project folder\n",
    "\n",
    "# Create organized subdirectories\n",
    "PATHS = {\n",
    "    'data': f'{BASE_DIR}/data',\n",
    "    'models': f'{BASE_DIR}/models',\n",
    "    'checkpoints': f'{BASE_DIR}/checkpoints',\n",
    "    'results': f'{BASE_DIR}/results',\n",
    "    'logs': f'{BASE_DIR}/logs'\n",
    "}\n",
    "\n",
    "# Create all directories\n",
    "for path in PATHS.values():\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Update data paths\n",
    "images_path = os.path.join(PATHS['data'], 'PENGWIN_CT_train_images')\n",
    "labels_path = os.path.join(PATHS['data'], 'PENGWIN_CT_train_labels')\n",
    "\n",
    "# Update model save paths\n",
    "MODEL_SAVE_PATH = os.path.join(PATHS['models'], 'best_unet_model.pth')\n",
    "CHECKPOINT_DIR = PATHS['checkpoints']\n",
    "\n",
    "# List and count files\n",
    "image_files = sorted([f for f in os.listdir(images_path) if f.endswith('.mha')])\n",
    "label_files = sorted([f for f in os.listdir(labels_path) if f.endswith('.mha')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def extract_patches(self, image, label):\n",
    "        \"\"\"Extract patches from image and label\"\"\"\n",
    "        patches_img = []\n",
    "        patches_label = []\n",
    "        \n",
    "        D, H, W = image.shape\n",
    "        \n",
    "        # Calculate steps for each dimension\n",
    "        d_steps = range(0, D - self.patch_size[0] + 1, self.stride[0])\n",
    "        h_steps = range(0, H - self.patch_size[1] + 1, self.stride[1])\n",
    "        w_steps = range(0, W - self.patch_size[2] + 1, self.stride[2])\n",
    "        \n",
    "        # If image is smaller than patch size, pad it\n",
    "        if D < self.patch_size[0]:\n",
    "            d_steps = [0]\n",
    "        if H < self.patch_size[1]:\n",
    "            h_steps = [0]\n",
    "        if W < self.patch_size[2]:\n",
    "            w_steps = [0]\n",
    "            \n",
    "        for d in d_steps:\n",
    "            for h in h_steps:\n",
    "                for w in w_steps:\n",
    "                    # Extract patches\n",
    "                    d_end = min(d + self.patch_size[0], D)\n",
    "                    h_end = min(h + self.patch_size[1], H)\n",
    "                    w_end = min(w + self.patch_size[2], W)\n",
    "                    \n",
    "                    patch_img = image[d:d_end, h:h_end, w:w_end]\n",
    "                    patch_label = label[d:d_end, h:h_end, w:w_end]\n",
    "                    \n",
    "                    # Pad if necessary\n",
    "                    if patch_img.shape != self.patch_size:\n",
    "                        pad_d = self.patch_size[0] - patch_img.shape[0]\n",
    "                        pad_h = self.patch_size[1] - patch_img.shape[1]\n",
    "                        pad_w = self.patch_size[2] - patch_img.shape[2]\n",
    "                        \n",
    "                        patch_img = np.pad(patch_img, \n",
    "                                         ((0, pad_d), (0, pad_h), (0, pad_w)), \n",
    "                                         mode='constant')\n",
    "                        patch_label = np.pad(patch_label, \n",
    "                                           ((0, pad_d), (0, pad_h), (0, pad_w)), \n",
    "                                           mode='constant')\n",
    "                    \n",
    "                    patches_img.append(patch_img)\n",
    "                    patches_label.append(patch_label)\n",
    "        \n",
    "        return np.array(patches_img), np.array(patches_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and label\n",
    "        image = sitk.GetArrayFromImage(sitk.ReadImage(self.image_paths[idx])).astype(np.float32)\n",
    "        label = sitk.GetArrayFromImage(sitk.ReadImage(self.label_paths[idx])).astype(np.float32)\n",
    "        \n",
    "        # Preprocessing\n",
    "        image = np.clip(image, -1000, 1000)\n",
    "        image = (image + 1000) / 2000\n",
    "        label = (label > 0).astype(np.float32)\n",
    "        \n",
    "        # Extract patches\n",
    "        patches_img, patches_label = self.extract_patches(image, label)\n",
    "        \n",
    "        # Randomly select one patch during training\n",
    "        patch_idx = np.random.randint(len(patches_img))\n",
    "        \n",
    "        # Add channel dimension\n",
    "        image_patch = np.expand_dims(patches_img[patch_idx], axis=0)\n",
    "        label_patch = np.expand_dims(patches_label[patch_idx], axis=0)\n",
    "        \n",
    "        return torch.tensor(image_patch), torch.tensor(label_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_image_path, val_label_path, patch_size=(128, 128, 128)):\n",
    "    \"\"\"\n",
    "    Validate model performance on a single validation image\n",
    "    Returns Dice score\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Load validation image and label\n",
    "    val_image = sitk.GetArrayFromImage(sitk.ReadImage(val_image_path)).astype(np.float32)\n",
    "    val_label = sitk.GetArrayFromImage(sitk.ReadImage(val_label_path)).astype(np.float32)\n",
    "    \n",
    "    # Preprocess\n",
    "    val_image = np.clip(val_image, -1000, 1000)\n",
    "    val_image = (val_image + 1000) / 2000\n",
    "    val_label = (val_label > 0).astype(np.float32)\n",
    "    \n",
    "    # Predict using sliding window\n",
    "    prediction = predict_volume(model, val_image, patch_size=patch_size)\n",
    "    \n",
    "    # Apply sigmoid and threshold\n",
    "    prediction = (prediction > 0.5).astype(np.float32)\n",
    "    \n",
    "    # Calculate Dice score\n",
    "    intersection = np.sum(prediction * val_label)\n",
    "    dice_score = (2. * intersection) / (np.sum(prediction) + np.sum(val_label) + 1e-7)\n",
    "    \n",
    "    return dice_score\n",
    "\n",
    "def predict_volume(model, image, patch_size=(128, 128, 128), stride=(64, 64, 64)):\n",
    "    \"\"\"Predict segmentation using sliding window approach\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    D, H, W = image.shape\n",
    "    output = np.zeros_like(image, dtype=np.float32)\n",
    "    weight = np.zeros_like(image, dtype=np.float32)\n",
    "    \n",
    "    # Calculate steps\n",
    "    d_steps = range(0, D - patch_size[0] + 1, stride[0])\n",
    "    h_steps = range(0, H - patch_size[1] + 1, stride[1])\n",
    "    w_steps = range(0, W - patch_size[2] + 1, stride[2])\n",
    "    \n",
    "    # Handle edge cases\n",
    "    if D < patch_size[0]:\n",
    "        d_steps = [0]\n",
    "    if H < patch_size[1]:\n",
    "        h_steps = [0]\n",
    "    if W < patch_size[2]:\n",
    "        w_steps = [0]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d in d_steps:\n",
    "            for h in h_steps:\n",
    "                for w in w_steps:\n",
    "                    # Extract patch\n",
    "                    d_end = min(d + patch_size[0], D)\n",
    "                    h_end = min(h + patch_size[1], H)\n",
    "                    w_end = min(w + patch_size[2], W)\n",
    "                    \n",
    "                    patch = image[d:d_end, h:h_end, w:w_end]\n",
    "                    \n",
    "                    # Pad if necessary\n",
    "                    if patch.shape != patch_size:\n",
    "                        pad_d = patch_size[0] - patch.shape[0]\n",
    "                        pad_h = patch_size[1] - patch.shape[1]\n",
    "                        pad_w = patch_size[2] - patch.shape[2]\n",
    "                        \n",
    "                        patch = np.pad(patch, \n",
    "                                     ((0, pad_d), (0, pad_h), (0, pad_w)), \n",
    "                                     mode='constant')\n",
    "                    \n",
    "                    # Predict\n",
    "                    patch = torch.tensor(patch).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "                    pred = torch.sigmoid(model(patch)).cpu().numpy()[0, 0]\n",
    "                    \n",
    "                    # Unpad if necessary\n",
    "                    if pad_d > 0 or pad_h > 0 or pad_w > 0:\n",
    "                        pred = pred[:d_end-d, :h_end-h, :w_end-w]\n",
    "                    \n",
    "                    # Add to output with weight\n",
    "                    output[d:d_end, h:h_end, w:w_end] += pred\n",
    "                    weight[d:d_end, h:h_end, w:w_end] += 1\n",
    "    \n",
    "    # Average overlapping predictions\n",
    "    output = np.divide(output, weight, where=weight!=0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeModelEstimator:\n",
    "    def __init__(self, n_components=10):\n",
    "        self.n_components = n_components\n",
    "        self.pca = None\n",
    "        self.mean_shape = None\n",
    "        \n",
    "    def fit(self, training_masks):\n",
    "        \"\"\"训练形状模型\"\"\"\n",
    "        # 将掩码转换为距离图\n",
    "        distance_maps = []\n",
    "        for mask in training_masks:\n",
    "            dist_map = self._compute_distance_map(mask)\n",
    "            distance_maps.append(dist_map.flatten())\n",
    "            \n",
    "        # 执行PCA\n",
    "        self.pca = PCA(n_components=self.n_components)\n",
    "        self.pca.fit(distance_maps)\n",
    "        self.mean_shape = self.pca.mean_.reshape(mask.shape)\n",
    "        \n",
    "    def estimate_shape(self, segmentation, alpha=1.0, beta=1.0, gamma=0.1):\n",
    "        \"\"\"使用level set方法估计形状\"\"\"\n",
    "        phi = self._compute_distance_map(segmentation)\n",
    "        \n",
    "        for _ in range(50):  # 迭代优化\n",
    "            # 计算图像力\n",
    "            F = self._compute_image_force(segmentation)\n",
    "            \n",
    "            # 计算形状模型力\n",
    "            shape_force = self._compute_shape_force(phi)\n",
    "            \n",
    "            # 计算曲率\n",
    "            curvature = self._compute_curvature(phi)\n",
    "            \n",
    "            # 更新level set\n",
    "            dphi = (alpha * F + beta * shape_force + gamma * curvature)\n",
    "            phi += dphi\n",
    "            \n",
    "        return phi\n",
    "    \n",
    "    def _compute_distance_map(self, binary_mask):\n",
    "        \"\"\"计算二值掩码的距离图\"\"\"\n",
    "        return ndimage.distance_transform_edt(binary_mask) - \\\n",
    "               ndimage.distance_transform_edt(1 - binary_mask)\n",
    "    \n",
    "    def _compute_image_force(self, seg):\n",
    "        \"\"\"计算图像力\"\"\"\n",
    "        return ndimage.gaussian_gradient_magnitude(seg, sigma=1)\n",
    "    \n",
    "    def _compute_shape_force(self, phi):\n",
    "        \"\"\"计算形状模型力\"\"\"\n",
    "        # 投影到PCA空间并重建\n",
    "        flat_phi = phi.flatten()\n",
    "        projection = self.pca.transform([flat_phi])[0]\n",
    "        reconstruction = self.pca.inverse_transform([projection])[0]\n",
    "        return reconstruction.reshape(phi.shape) - phi\n",
    "    \n",
    "    def _compute_curvature(self, phi):\n",
    "        \"\"\"计算曲率\"\"\"\n",
    "        return ndimage.laplace(phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTScanDataset(Dataset):\n",
    "    def __init__(self, images_path, labels_path, patch_size=(128, 128, 72), \n",
    "                 shape_context=None, transform=True):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.transform = transform\n",
    "        self.shape_context = shape_context\n",
    "        \n",
    "        # 加载数据\n",
    "        self.images = self._load_images(images_path)\n",
    "        self.labels = self._load_labels(labels_path)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # 获取patch\n",
    "        image_patch, label_patch = self._get_patch(image, label)\n",
    "        \n",
    "        # 数据增强\n",
    "        if self.transform:\n",
    "            image_patch, label_patch = self._augment(image_patch, label_patch)\n",
    "        \n",
    "        # 添加形状上下文通道\n",
    "        if self.shape_context is not None:\n",
    "            shape_context_patch = self._get_shape_context_patch(idx)\n",
    "            image_patch = np.concatenate([image_patch, shape_context_patch], axis=0)\n",
    "        else:\n",
    "            # 添加空白形状上下文\n",
    "            blank_context = np.zeros((2,) + image_patch.shape[1:], dtype=np.float32)\n",
    "            image_patch = np.concatenate([image_patch, blank_context], axis=0)\n",
    "        \n",
    "        return torch.from_numpy(image_patch), torch.from_numpy(label_patch)\n",
    "    \n",
    "    def _augment(self, image, label):\n",
    "        \"\"\"实现数据增强\"\"\"\n",
    "        # 随机平移\n",
    "        shift = np.random.randint(-20, 20, size=3)\n",
    "        image = ndimage.shift(image, shift, mode='constant')\n",
    "        label = ndimage.shift(label, shift, mode='constant')\n",
    "        \n",
    "        # 随机旋转\n",
    "        angle = np.random.randint(-15, 15)\n",
    "        image = ndimage.rotate(image, angle, axes=(1,2), mode='constant')\n",
    "        label = ndimage.rotate(label, angle, axes=(1,2), mode='constant')\n",
    "        \n",
    "        # 随机缩放\n",
    "        scale = np.random.uniform(0.9, 1.1)\n",
    "        image = ndimage.zoom(image, scale, mode='constant')\n",
    "        label = ndimage.zoom(label, scale, mode='constant')\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=6):  # 3 input channels (1 CT + 2 shape), 6 classes\n",
    "        super(UNet3D, self).__init__()\n",
    "        \n",
    "        def conv_block(in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv3d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm3d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv3d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm3d(out_ch),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        \n",
    "        # 3 max pooling layers as mentioned in paper\n",
    "        self.encoder1 = conv_block(in_channels, 64)\n",
    "        self.encoder2 = conv_block(64, 128)\n",
    "        self.encoder3 = conv_block(128, 256)\n",
    "        \n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = conv_block(256, 512)\n",
    "        \n",
    "        # 3 up-sampling layers as mentioned in paper\n",
    "        self.upconv3 = nn.ConvTranspose3d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = conv_block(512, 256)  # 512 due to skip connection\n",
    "        self.upconv2 = nn.ConvTranspose3d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = conv_block(256, 128)  # 256 due to skip connection\n",
    "        self.upconv1 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = conv_block(128, 64)   # 128 due to skip connection\n",
    "        \n",
    "        self.final = nn.Conv3d(64, out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        e1 = self.encoder1(x)\n",
    "        e2 = self.encoder2(self.pool(e1))\n",
    "        e3 = self.encoder3(self.pool(e2))\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(self.pool(e3))\n",
    "        \n",
    "        # Decoding with skip connections\n",
    "        d3 = self.upconv3(b)\n",
    "        d3 = self.decoder3(torch.cat([d3, e3], dim=1))\n",
    "        d2 = self.upconv2(d3)\n",
    "        d2 = self.decoder2(torch.cat([d2, e2], dim=1))\n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = self.decoder1(torch.cat([d1, e1], dim=1))\n",
    "        \n",
    "        return self.final(d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show the parameter of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture:\n",
      "------------------\n",
      "encoder1.0.weight: [64, 1, 3, 3, 3] (1,728 parameters)\n",
      "encoder1.0.bias: [64] (64 parameters)\n",
      "encoder1.2.weight: [64, 64, 3, 3, 3] (110,592 parameters)\n",
      "encoder1.2.bias: [64] (64 parameters)\n",
      "encoder2.0.weight: [128, 64, 3, 3, 3] (221,184 parameters)\n",
      "encoder2.0.bias: [128] (128 parameters)\n",
      "encoder2.2.weight: [128, 128, 3, 3, 3] (442,368 parameters)\n",
      "encoder2.2.bias: [128] (128 parameters)\n",
      "encoder3.0.weight: [256, 128, 3, 3, 3] (884,736 parameters)\n",
      "encoder3.0.bias: [256] (256 parameters)\n",
      "encoder3.2.weight: [256, 256, 3, 3, 3] (1,769,472 parameters)\n",
      "encoder3.2.bias: [256] (256 parameters)\n",
      "encoder4.0.weight: [512, 256, 3, 3, 3] (3,538,944 parameters)\n",
      "encoder4.0.bias: [512] (512 parameters)\n",
      "encoder4.2.weight: [512, 512, 3, 3, 3] (7,077,888 parameters)\n",
      "encoder4.2.bias: [512] (512 parameters)\n",
      "bottleneck.0.weight: [1024, 512, 3, 3, 3] (14,155,776 parameters)\n",
      "bottleneck.0.bias: [1024] (1,024 parameters)\n",
      "bottleneck.2.weight: [1024, 1024, 3, 3, 3] (28,311,552 parameters)\n",
      "bottleneck.2.bias: [1024] (1,024 parameters)\n",
      "upconv4.weight: [1024, 512, 2, 2, 2] (4,194,304 parameters)\n",
      "upconv4.bias: [512] (512 parameters)\n",
      "decoder4.0.weight: [512, 1024, 3, 3, 3] (14,155,776 parameters)\n",
      "decoder4.0.bias: [512] (512 parameters)\n",
      "decoder4.2.weight: [512, 512, 3, 3, 3] (7,077,888 parameters)\n",
      "decoder4.2.bias: [512] (512 parameters)\n",
      "upconv3.weight: [512, 256, 2, 2, 2] (1,048,576 parameters)\n",
      "upconv3.bias: [256] (256 parameters)\n",
      "decoder3.0.weight: [256, 512, 3, 3, 3] (3,538,944 parameters)\n",
      "decoder3.0.bias: [256] (256 parameters)\n",
      "decoder3.2.weight: [256, 256, 3, 3, 3] (1,769,472 parameters)\n",
      "decoder3.2.bias: [256] (256 parameters)\n",
      "upconv2.weight: [256, 128, 2, 2, 2] (262,144 parameters)\n",
      "upconv2.bias: [128] (128 parameters)\n",
      "decoder2.0.weight: [128, 256, 3, 3, 3] (884,736 parameters)\n",
      "decoder2.0.bias: [128] (128 parameters)\n",
      "decoder2.2.weight: [128, 128, 3, 3, 3] (442,368 parameters)\n",
      "decoder2.2.bias: [128] (128 parameters)\n",
      "upconv1.weight: [128, 64, 2, 2, 2] (65,536 parameters)\n",
      "upconv1.bias: [64] (64 parameters)\n",
      "decoder1.0.weight: [64, 128, 3, 3, 3] (221,184 parameters)\n",
      "decoder1.0.bias: [64] (64 parameters)\n",
      "decoder1.2.weight: [64, 64, 3, 3, 3] (110,592 parameters)\n",
      "decoder1.2.bias: [64] (64 parameters)\n",
      "output.weight: [1, 64, 1, 1, 1] (64 parameters)\n",
      "output.bias: [1] (1 parameters)\n",
      "\n",
      "Total parameters: 90,292,673\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    def print_model_summary(model):\n",
    "        \"\"\"Print model architecture summary\"\"\"\n",
    "        print(\"Model Architecture:\")\n",
    "        print(\"------------------\")\n",
    "        total_params = 0\n",
    "        for name, param in model.named_parameters():\n",
    "            param_count = param.numel()\n",
    "            total_params += param_count\n",
    "            print(f\"{name}: {list(param.shape)} ({param_count:,} parameters)\")\n",
    "        print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "    # Initialize model and print summary\n",
    "    model = UNet3D(in_channels=1, out_channels=1)\n",
    "    print_model_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Configuration and Progress Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Use paths from PATHS dictionary\n",
    "images_path = os.path.join(PATHS['data'], 'PENGWIN_CT_train_images')\n",
    "labels_path = os.path.join(PATHS['data'], 'PENGWIN_CT_train_labels')\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# Dataset and DataLoader setup\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: torch.tensor(x, dtype=torch.float32)),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Initialize dataset with patch-based approach\n",
    "train_dataset = CTScanDataset(\n",
    "    images_path=images_path,\n",
    "    labels_path=labels_path,\n",
    "    patch_size=(128, 128, 128),\n",
    "    stride=(64, 64, 64)\n",
    ")\n",
    "\n",
    "# Rest of your training code remains the same\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Model initialization\n",
    "model = UNet3D(in_channels=1, out_channels=1).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "class TrainingMonitor:\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "    def update(self, epoch_loss):\n",
    "        self.train_losses.append(epoch_loss)\n",
    "        self.current_epoch += 1\n",
    "        \n",
    "    def plot_progress(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.train_losses, label='Training Loss')\n",
    "        plt.title('Training Progress')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def print_stats(self):\n",
    "        print(f\"Current epoch: {self.current_epoch}\")\n",
    "        print(f\"Best loss: {min(self.train_losses):.4f}\")\n",
    "        print(f\"Current loss: {self.train_losses[-1]:.4f}\")\n",
    "\n",
    "# Initialize training monitor\n",
    "monitor = TrainingMonitor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:  92%|█████████▏| 46/50 [44:26<04:32, 68.08s/it, loss=0.318]  "
     ]
    }
   ],
   "source": [
    "# Update save paths at the start\n",
    "best_model_dice_path = os.path.join(PATHS['models'], 'best_model_dice.pth')\n",
    "best_model_loss_path = os.path.join(PATHS['models'], 'best_model_loss.pth')\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    \"\"\"两步训练过程\"\"\"\n",
    "    # Step 1: 使用空白形状上下文训练\n",
    "    print(\"Step 1: Training with blank shape context...\")\n",
    "    for epoch in range(100):  # 论文中提到100个epoch\n",
    "        train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        validate_model(model, val_loader, device)\n",
    "        \n",
    "    # 创建形状模型\n",
    "    shape_estimator = ShapeModelEstimator()\n",
    "    initial_predictions = get_predictions(model, train_loader, device)\n",
    "    shape_estimator.fit(initial_predictions)\n",
    "    \n",
    "    # Step 2: 使用形状上下文再训练\n",
    "    print(\"Step 2: Training with shape context...\")\n",
    "    for epoch in range(40):  # 论文中提到40个epoch\n",
    "        # 更新形状上下文\n",
    "        predictions = get_predictions(model, train_loader, device)\n",
    "        shape_contexts = shape_estimator.estimate_shape(predictions)\n",
    "        \n",
    "        # 更新数据集的形状上下文\n",
    "        train_loader.dataset.shape_context = shape_contexts\n",
    "        \n",
    "        # 训练\n",
    "        train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        # 在每个epoch后进行验证\n",
    "        validate_model(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, shape_estimator, device, max_iterations=3):\n",
    "    \"\"\"迭代测试过程\"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, _ in test_loader:\n",
    "            # 初始预测（使用空白形状上下文）\n",
    "            current_pred = model(images.to(device))\n",
    "            \n",
    "            # 迭代改进\n",
    "            for _ in range(max_iterations):\n",
    "                # 估计形状\n",
    "                shape_context = shape_estimator.estimate_shape(current_pred.cpu().numpy())\n",
    "                \n",
    "                # 将形状上下文添加到输入\n",
    "                shape_input = torch.from_numpy(shape_context).to(device)\n",
    "                combined_input = torch.cat([images, shape_input], dim=1)\n",
    "                \n",
    "                # 重新预测\n",
    "                new_pred = model(combined_input)\n",
    "                \n",
    "                # 检查收敛\n",
    "                if torch.abs(new_pred - current_pred).mean() < 1e-4:\n",
    "                    break\n",
    "                    \n",
    "                current_pred = new_pred\n",
    "            \n",
    "            results.append(current_pred)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMonitor:\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.val_dices = []\n",
    "        self.current_epoch = 0\n",
    "        self.plot_dir = PATHS['results']\n",
    "        \n",
    "    def plot_progress(self):\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Plot training loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.train_losses, label='Training Loss')\n",
    "        plt.title('Training Loss Over Time')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot validation dice if available\n",
    "        if self.val_dices:\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(range(0, len(self.val_dices)*5, 5), self.val_dices, label='Validation Dice')\n",
    "            plt.title('Validation Dice Over Time')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Dice Score')\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        plot_path = os.path.join(self.plot_dir, f'training_progress_epoch_{self.current_epoch:03d}.png')\n",
    "        plt.savefig(plot_path)\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pelvis_seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
